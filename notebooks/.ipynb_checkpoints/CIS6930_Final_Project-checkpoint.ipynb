{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b023f6d3-45b5-4ea7-aa50-4330dc438abe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cu128\n",
      "12.8\n",
      "True\n",
      "NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a821e524-e8e0-44f0-9217-bda5d0ac5c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths set.\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, json, re, random, math, time\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# === Paths (adjust if needed) ===\n",
    "PROJECT_ROOT = \"/home/cj.bowers/blue_cis6930/llama3_project\"\n",
    "DATA_JSONL   = f\"{PROJECT_ROOT}/data/ai_tutor_merged.jsonl\"\n",
    "CACHE_DIR    = f\"{PROJECT_ROOT}/.cache/huggingface\"\n",
    "OUTPUTS_DIR  = f\"{PROJECT_ROOT}/outputs\"\n",
    "MODELS_DIR   = f\"{PROJECT_ROOT}/models\"\n",
    "\n",
    "# Create dirs\n",
    "for p in [CACHE_DIR, OUTPUTS_DIR, MODELS_DIR]:\n",
    "    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Keep HF cache on Blue\n",
    "os.environ[\"HF_HOME\"] = CACHE_DIR\n",
    "\n",
    "# OPTIONAL (for gated models like Llama 3). If needed, uncomment and paste your token:\n",
    "# os.environ[\"HF_TOKEN\"] = \"hf_...\"\n",
    "\n",
    "# Base model (pick one you have access to)\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"   # ungated, no HF token needed\n",
    "EVAL_SAMPLES = 200                            # quick eval size (increase later if you want)\n",
    "\n",
    "print(\"Paths set.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06084a15-a175-45b6-aea9-a751a398ceeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " '###System: You are an expert AI tutor. Explain technical concepts clearly. ###User: You will be given a definition of a task first, then some input of the task.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def iter_jsonl(path, limit=None):\n",
    "    with open(path, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if limit is not None and i >= limit:\n",
    "                break\n",
    "            yield json.loads(line)\n",
    "\n",
    "ASSISTANT_MARK = \"###Assistant:\"\n",
    "USER_MARK = \"###User:\"\n",
    "\n",
    "def split_prompt_answer(example_text: str):\n",
    "    \"\"\"\n",
    "    Return (prompt_text_to_feed_model, target_answer).\n",
    "    We feed everything up to '###Assistant:' as the prompt context.\n",
    "    \"\"\"\n",
    "    idx = example_text.find(ASSISTANT_MARK)\n",
    "    if idx == -1:\n",
    "        return None, None\n",
    "    prompt_part = example_text[:idx + len(ASSISTANT_MARK)]\n",
    "    # The actual assistant target follows the marker\n",
    "    answer = example_text[idx + len(ASSISTANT_MARK):].strip()\n",
    "    return prompt_part.strip(), answer\n",
    "\n",
    "# Build a small eval set\n",
    "eval_rows = []\n",
    "for obj in iter_jsonl(DATA_JSONL, limit=20000):  # scan first chunk to sample diverse items\n",
    "    p, a = split_prompt_answer(obj[\"text\"])\n",
    "    if p and a:\n",
    "        eval_rows.append({\"prompt\": p, \"answer\": a})\n",
    "    if len(eval_rows) >= EVAL_SAMPLES:\n",
    "        break\n",
    "\n",
    "len(eval_rows), eval_rows[0][\"prompt\"][:160]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a79051-78d3-4625-836d-54c5e5ee6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Tokenizer & metrics/helpers (works with Mistral)\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Uses MODEL_ID from your config cell (e.g., \"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "ASSISTANT_MARK = \"###Assistant:\"\n",
    "\n",
    "def normalize_text(s: str):\n",
    "    return re.sub(r\"\\s+\", \" \", s.strip().lower())\n",
    "\n",
    "def token_f1(pred: str, gold: str):\n",
    "    p = normalize_text(pred).split()\n",
    "    g = normalize_text(gold).split()\n",
    "    if not p or not g: return 0.0\n",
    "    inter = sum(min(p.count(w), g.count(w)) for w in set(p))\n",
    "    if inter == 0: return 0.0\n",
    "    precision = inter / len(p)\n",
    "    recall    = inter / len(g)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def _lcs_len(a, b):\n",
    "    n, m = len(a), len(b)\n",
    "    dp = [[0]*(m+1) for _ in range(n+1)]\n",
    "    for i in range(n):\n",
    "        ai = a[i]\n",
    "        dpi1 = dp[i+1]\n",
    "        dpi  = dp[i]\n",
    "        for j in range(m):\n",
    "            dpi1[j+1] = dpi[j] + 1 if ai == b[j] else (dpi1[j] if dpi1[j] > dpi[j+1] else dpi[j+1])\n",
    "    return dp[n][m]\n",
    "\n",
    "def rouge_l(pred: str, gold: str):\n",
    "    P, G = normalize_text(pred).split(), normalize_text(gold).split()\n",
    "    if not P or not G: return 0.0\n",
    "    L = _lcs_len(P, G)\n",
    "    prec, rec = L/len(P), L/len(G)\n",
    "    return 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
    "\n",
    "def reduce_scores(preds, refs):\n",
    "    f1s = [token_f1(p, r) for p, r in zip(preds, refs)]\n",
    "    rls = [rouge_l(p, r) for p, r in zip(preds, refs)]\n",
    "    return {\n",
    "        \"count\": len(f1s),\n",
    "        \"token_f1_mean\": sum(f1s)/len(f1s) if f1s else 0.0,\n",
    "        \"rougeL_mean\":   sum(rls)/len(rls) if rls else 0.0,\n",
    "    }\n",
    "\n",
    "def generate_batch(prompts, model, max_new_tokens=256, temperature=0.0):\n",
    "    \"\"\"Generate completions and return ONLY the text after the last '###Assistant:' marker.\"\"\"\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(\n",
    "        next(model.parameters()).device\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=(temperature > 0),\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    completions = []\n",
    "    for full in texts:\n",
    "        idx = full.rfind(ASSISTANT_MARK)\n",
    "        completions.append(full[idx+len(ASSISTANT_MARK):].strip() if idx != -1 else full.strip())\n",
    "    return completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ca904b-ef2e-442f-ba05-b9a5d962de3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780e09621c384b6b89435f9b30de3859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2bc5d727ab4c56ac82e3d11f04821b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Baseline eval:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fast baseline done: {'count': 20, 'token_f1_mean': 0.2859335632238266, 'rougeL_mean': 0.21637248040869558}\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — FAST baseline (Mistral) so the cell completes quickly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import transformers\n",
    "\n",
    "transformers.logging.set_verbosity_error()  # quiet the padding/temp warnings\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"   # decoder-only: left padding\n",
    "\n",
    "# model\n",
    "base_eval_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "# --- make a tiny eval slice so this finishes fast ---\n",
    "EVAL_SLICE = 20                   # <-- change to 100/200 later if you want\n",
    "rows = eval_rows[:EVAL_SLICE]\n",
    "\n",
    "preds_base, golds = [], []\n",
    "BATCH = 1                         # safer on VRAM; bump to 2 later\n",
    "MAX_NEW = 64                      # quicker gen; bump to 128–256 later\n",
    "\n",
    "for i in tqdm(range(0, len(rows), BATCH), desc=\"Baseline eval\"):\n",
    "    batch = rows[i:i+BATCH]\n",
    "    batch_prompts = [r[\"prompt\"] for r in batch]\n",
    "    batch_refs    = [r[\"answer\"] for r in batch]\n",
    "\n",
    "    # generate_batch uses no_grad; just ensure model is eval()\n",
    "    preds = generate_batch(batch_prompts, base_eval_model,\n",
    "                           max_new_tokens=MAX_NEW, temperature=0.0)\n",
    "    preds_base.extend(preds)\n",
    "    golds.extend(batch_refs)\n",
    "\n",
    "baseline_metrics = reduce_scores(preds_base, golds)\n",
    "print(\"✅ Fast baseline done:\", baseline_metrics)\n",
    "\n",
    "# Keep these globals for later cells:\n",
    "# - baseline_metrics\n",
    "# - golds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ee41e1-f25e-4f41-a4b0-cf3c18f19534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2987a7f5f84e30bc4799ce23f62e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trainer ready (Mistral + QLoRA 4-bit, fp16 compute, HF Trainer)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/18245251/ipykernel_3739132/470555978.py:111: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — Mistral training model init (QLoRA 4-bit, fp16 compute, HF Trainer)\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, List, Dict\n",
    "import os, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    TrainingArguments, AutoModelForCausalLM, Trainer, AutoTokenizer, BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# --- env tweaks: avoid deepspeed, reduce fragmentation ---\n",
    "os.environ[\"ACCELERATE_USE_DEEPSPEED\"] = \"0\"\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "# assumes MODEL_ID, DATA_JSONL, OUTPUTS_DIR, and tokenizer already exist\n",
    "# if tokenizer isn't defined yet, uncomment the next line:\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, padding_side=\"left\")\n",
    "\n",
    "# --- 4-bit quantization config (QLoRA) ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # fp16 compute on L4\n",
    ")\n",
    "\n",
    "# --- Load base model quantized + automatically placed on GPU ---\n",
    "train_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,                      # e.g., \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",             # safe now that we’re not using meta tensors\n",
    "    low_cpu_mem_usage=False        # keep False to avoid lingering 'meta'\n",
    ")\n",
    "train_model.config.use_cache = False\n",
    "train_model.gradient_checkpointing_enable()\n",
    "# needed for QLoRA so gradients flow to inputs with 4-bit weights\n",
    "if hasattr(train_model, \"enable_input_require_grads\"):\n",
    "    train_model.enable_input_require_grads()\n",
    "\n",
    "# --- LoRA config (good defaults for Mistral 7B) ---\n",
    "lora_cfg = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "train_model = get_peft_model(train_model, lora_cfg)\n",
    "\n",
    "# --- Dataset (your merged JSONL) ---\n",
    "train_ds = load_dataset(\"json\", data_files=DATA_JSONL, split=\"train\")\n",
    "\n",
    "# --- Loss mask: ignore everything before \"###Assistant:\" ---\n",
    "ASSISTANT_MARK = \"###Assistant:\"\n",
    "assistant_ids = tokenizer(ASSISTANT_MARK, add_special_tokens=False).input_ids\n",
    "\n",
    "@dataclass\n",
    "class AssistantOnlyCollator:\n",
    "    tokenizer: Any\n",
    "    max_length: int = 1024  # you can raise to 2048 later if VRAM allows\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]):\n",
    "        texts = [f[\"text\"] for f in features]\n",
    "        batch = self.tokenizer(\n",
    "            texts,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "        # mask tokens before the start of Assistant span\n",
    "        for i, ids in enumerate(batch[\"input_ids\"]):\n",
    "            start = -1\n",
    "            for j in range(len(ids) - len(assistant_ids) + 1):\n",
    "                if torch.equal(ids[j:j+len(assistant_ids)],\n",
    "                               torch.tensor(assistant_ids, device=ids.device)):\n",
    "                    start = j + len(assistant_ids)\n",
    "                    break\n",
    "            if start > 0:\n",
    "                labels[i, :start] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "collator = AssistantOnlyCollator(tokenizer=tokenizer, max_length=1024)\n",
    "\n",
    "# --- Training arguments (tuned for 4-bit + LoRA on L4) ---\n",
    "out_dir = f\"{OUTPUTS_DIR}/mistral_lora_ai_tutor\"\n",
    "args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=20,\n",
    "    save_steps=1000,\n",
    "    fp16=True,                  # fp16 compute is fine with 4-bit\n",
    "    bf16=False,                 # keep False unless you specifically want bf16\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=True,\n",
    ")\n",
    "\n",
    "# --- Standard HF Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=train_model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer ready (Mistral + QLoRA 4-bit, fp16 compute, HF Trainer)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d01fa6c7-8d50-4610-8a89-746c870ae011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1154, 'grad_norm': 0.7724497318267822, 'learning_rate': 2.2975996130358548e-07, 'epoch': 3.627967365526555e-05}\n",
      "{'loss': 1.2142, 'grad_norm': 0.7217940092086792, 'learning_rate': 4.7161255214946496e-07, 'epoch': 7.25593473105311e-05}\n",
      "{'loss': 1.2568, 'grad_norm': 1.7581788301467896, 'learning_rate': 7.134651429953444e-07, 'epoch': 0.00010883902096579666}\n",
      "{'loss': 1.1551, 'grad_norm': 0.9755426049232483, 'learning_rate': 1.1971703246871033e-06, 'epoch': 0.00018139836827632777}\n",
      "{'loss': 1.0188, 'grad_norm': 0.9482971429824829, 'learning_rate': 1.4390229155329827e-06, 'epoch': 0.00021767804193159333}\n",
      "{'loss': 1.2063, 'grad_norm': 0.8276782631874084, 'learning_rate': 1.6808755063788623e-06, 'epoch': 0.0002539577155868589}\n",
      "{'loss': 1.0144, 'grad_norm': 0.71774822473526, 'learning_rate': 1.9227280972247415e-06, 'epoch': 0.0002902373892421244}\n",
      "{'loss': 1.1212, 'grad_norm': 1.069435954093933, 'learning_rate': 2.152488058528327e-06, 'epoch': 0.00032651706289738995}\n",
      "{'loss': 1.0854, 'grad_norm': 0.8914446234703064, 'learning_rate': 2.3943406493742065e-06, 'epoch': 0.00036279673655265554}\n",
      "{'loss': 1.068, 'grad_norm': 0.7263368368148804, 'learning_rate': 2.6361932402200857e-06, 'epoch': 0.00039907641020792107}\n",
      "{'loss': 0.9809, 'grad_norm': 1.7102127075195312, 'learning_rate': 2.8780458310659654e-06, 'epoch': 0.00043535608386318666}\n",
      "{'loss': 0.9644, 'grad_norm': 0.6027331352233887, 'learning_rate': 3.119898421911845e-06, 'epoch': 0.0004716357575184522}\n",
      "{'loss': 0.9403, 'grad_norm': 0.6510177254676819, 'learning_rate': 3.3617510127577247e-06, 'epoch': 0.0005079154311737178}\n",
      "{'loss': 1.0584, 'grad_norm': 0.6254239082336426, 'learning_rate': 3.603603603603604e-06, 'epoch': 0.0005441951048289833}\n",
      "{'loss': 0.9263, 'grad_norm': 0.8503450155258179, 'learning_rate': 3.845456194449483e-06, 'epoch': 0.0005804747784842488}\n",
      "{'loss': 0.8094, 'grad_norm': 1.0484035015106201, 'learning_rate': 4.087308785295363e-06, 'epoch': 0.0006167544521395144}\n",
      "{'loss': 0.7785, 'grad_norm': 0.9369311332702637, 'learning_rate': 4.329161376141242e-06, 'epoch': 0.0006530341257947799}\n",
      "{'loss': 0.8257, 'grad_norm': 1.028626799583435, 'learning_rate': 4.571013966987121e-06, 'epoch': 0.0006893137994500455}\n",
      "{'loss': 0.8447, 'grad_norm': 0.7677721977233887, 'learning_rate': 4.812866557833001e-06, 'epoch': 0.0007255934731053111}\n",
      "{'loss': 0.8128, 'grad_norm': 0.77057284116745, 'learning_rate': 5.0547191486788805e-06, 'epoch': 0.0007618731467605766}\n",
      "{'loss': 0.8156, 'grad_norm': 0.5265840291976929, 'learning_rate': 5.296571739524759e-06, 'epoch': 0.0007981528204158421}\n",
      "{'loss': 0.7995, 'grad_norm': 0.8586996793746948, 'learning_rate': 5.53842433037064e-06, 'epoch': 0.0008344324940711077}\n",
      "{'loss': 0.8903, 'grad_norm': 0.8069478273391724, 'learning_rate': 5.7802769212165185e-06, 'epoch': 0.0008707121677263733}\n",
      "{'loss': 0.8019, 'grad_norm': 1.1059026718139648, 'learning_rate': 6.022129512062398e-06, 'epoch': 0.0009069918413816388}\n",
      "{'loss': 0.8588, 'grad_norm': 0.7389861941337585, 'learning_rate': 6.263982102908278e-06, 'epoch': 0.0009432715150369044}\n",
      "{'loss': 0.775, 'grad_norm': 0.633268415927887, 'learning_rate': 6.5058346937541575e-06, 'epoch': 0.00097955118869217}\n",
      "{'loss': 0.8838, 'grad_norm': 0.8412957191467285, 'learning_rate': 6.747687284600036e-06, 'epoch': 0.0010158308623474355}\n",
      "{'loss': 0.7857, 'grad_norm': 0.7395286560058594, 'learning_rate': 6.989539875445916e-06, 'epoch': 0.001052110536002701}\n",
      "{'loss': 0.7125, 'grad_norm': 0.8554545044898987, 'learning_rate': 7.231392466291796e-06, 'epoch': 0.0010883902096579666}\n",
      "{'loss': 0.7492, 'grad_norm': 0.6869778633117676, 'learning_rate': 7.473245057137674e-06, 'epoch': 0.001124669883313232}\n",
      "{'loss': 0.8383, 'grad_norm': 0.5836214423179626, 'learning_rate': 7.715097647983554e-06, 'epoch': 0.0011609495569684977}\n",
      "{'loss': 0.8376, 'grad_norm': 0.8261087536811829, 'learning_rate': 7.956950238829435e-06, 'epoch': 0.0011972292306237633}\n",
      "{'loss': 0.7793, 'grad_norm': 0.6476980447769165, 'learning_rate': 8.198802829675313e-06, 'epoch': 0.0012335089042790287}\n",
      "{'loss': 0.7934, 'grad_norm': 0.9019754528999329, 'learning_rate': 8.440655420521193e-06, 'epoch': 0.0012697885779342944}\n",
      "{'loss': 0.8189, 'grad_norm': 0.6048453450202942, 'learning_rate': 8.682508011367073e-06, 'epoch': 0.0013060682515895598}\n",
      "{'loss': 0.8518, 'grad_norm': 1.5238913297653198, 'learning_rate': 8.924360602212952e-06, 'epoch': 0.0013423479252448254}\n",
      "{'loss': 0.8909, 'grad_norm': 0.6418531537055969, 'learning_rate': 9.166213193058832e-06, 'epoch': 0.001378627598900091}\n",
      "{'loss': 0.8102, 'grad_norm': 0.7762534618377686, 'learning_rate': 9.408065783904711e-06, 'epoch': 0.0014149072725553565}\n",
      "{'loss': 0.9254, 'grad_norm': 1.1397713422775269, 'learning_rate': 9.64991837475059e-06, 'epoch': 0.0014511869462106222}\n",
      "{'loss': 0.781, 'grad_norm': 1.0024694204330444, 'learning_rate': 9.891770965596469e-06, 'epoch': 0.0014874666198658876}\n",
      "{'loss': 0.8119, 'grad_norm': 0.9939219951629639, 'learning_rate': 1.0133623556442349e-05, 'epoch': 0.0015237462935211532}\n",
      "{'loss': 0.7112, 'grad_norm': 0.8699257969856262, 'learning_rate': 1.0375476147288228e-05, 'epoch': 0.0015600259671764189}\n",
      "{'loss': 0.8987, 'grad_norm': 0.8158831000328064, 'learning_rate': 1.0617328738134108e-05, 'epoch': 0.0015963056408316843}\n",
      "{'loss': 0.8642, 'grad_norm': 0.4283788502216339, 'learning_rate': 1.0859181328979988e-05, 'epoch': 0.00163258531448695}\n",
      "{'loss': 0.7923, 'grad_norm': 1.2212125062942505, 'learning_rate': 1.1101033919825866e-05, 'epoch': 0.0016688649881422153}\n",
      "{'loss': 0.7435, 'grad_norm': 0.902820885181427, 'learning_rate': 1.1342886510671747e-05, 'epoch': 0.001705144661797481}\n",
      "{'loss': 0.7167, 'grad_norm': 1.2550240755081177, 'learning_rate': 1.1584739101517625e-05, 'epoch': 0.0017414243354527466}\n",
      "{'loss': 0.722, 'grad_norm': 1.2083159685134888, 'learning_rate': 1.1826591692363505e-05, 'epoch': 0.001777704009108012}\n",
      "{'loss': 0.7259, 'grad_norm': 1.0871529579162598, 'learning_rate': 1.2068444283209384e-05, 'epoch': 0.0018139836827632777}\n",
      "{'loss': 0.791, 'grad_norm': 0.7408707737922668, 'learning_rate': 1.2310296874055264e-05, 'epoch': 0.0018502633564185431}\n",
      "{'loss': 0.9354, 'grad_norm': 0.5360077023506165, 'learning_rate': 1.2552149464901145e-05, 'epoch': 0.0018865430300738088}\n",
      "{'loss': 0.7968, 'grad_norm': 0.49661746621131897, 'learning_rate': 1.2794002055747023e-05, 'epoch': 0.0019228227037290742}\n",
      "{'loss': 0.8358, 'grad_norm': 0.7804002165794373, 'learning_rate': 1.3035854646592901e-05, 'epoch': 0.00195910237738434}\n",
      "{'loss': 0.7778, 'grad_norm': 0.7628487944602966, 'learning_rate': 1.3277707237438782e-05, 'epoch': 0.0019953820510396052}\n",
      "{'loss': 0.9297, 'grad_norm': 0.8440874218940735, 'learning_rate': 1.3519559828284662e-05, 'epoch': 0.002031661724694871}\n",
      "{'loss': 0.7864, 'grad_norm': 0.5870585441589355, 'learning_rate': 1.376141241913054e-05, 'epoch': 0.0020679413983501365}\n",
      "{'loss': 0.7147, 'grad_norm': 1.075474500656128, 'learning_rate': 1.4003265009976421e-05, 'epoch': 0.002104221072005402}\n",
      "{'loss': 0.8208, 'grad_norm': 1.0079610347747803, 'learning_rate': 1.42451176008223e-05, 'epoch': 0.002140500745660668}\n",
      "{'loss': 0.7261, 'grad_norm': 0.6936157941818237, 'learning_rate': 1.4486970191668179e-05, 'epoch': 0.0021767804193159332}\n",
      "{'loss': 0.6938, 'grad_norm': 0.9206682443618774, 'learning_rate': 1.472882278251406e-05, 'epoch': 0.0022130600929711986}\n",
      "{'loss': 0.722, 'grad_norm': 0.7365866899490356, 'learning_rate': 1.4970675373359938e-05, 'epoch': 0.002249339766626464}\n",
      "{'loss': 0.7928, 'grad_norm': 0.8404042720794678, 'learning_rate': 1.5212527964205816e-05, 'epoch': 0.00228561944028173}\n",
      "{'loss': 0.6525, 'grad_norm': 1.2929531335830688, 'learning_rate': 1.54543805550517e-05, 'epoch': 0.0023218991139369954}\n",
      "{'loss': 0.6438, 'grad_norm': 1.5131235122680664, 'learning_rate': 1.5696233145897577e-05, 'epoch': 0.0023581787875922608}\n",
      "{'loss': 0.8515, 'grad_norm': 1.500851035118103, 'learning_rate': 1.5938085736743455e-05, 'epoch': 0.0023944584612475266}\n",
      "{'loss': 0.6829, 'grad_norm': 0.9268755912780762, 'learning_rate': 1.6179938327589336e-05, 'epoch': 0.002430738134902792}\n",
      "{'loss': 0.7192, 'grad_norm': 0.9813329577445984, 'learning_rate': 1.6421790918435214e-05, 'epoch': 0.0024670178085580575}\n",
      "{'loss': 0.7664, 'grad_norm': 0.8494693040847778, 'learning_rate': 1.6663643509281092e-05, 'epoch': 0.002503297482213323}\n",
      "{'loss': 0.732, 'grad_norm': 1.078651785850525, 'learning_rate': 1.6905496100126974e-05, 'epoch': 0.0025395771558685888}\n",
      "{'loss': 0.8535, 'grad_norm': 0.6992924809455872, 'learning_rate': 1.714734869097285e-05, 'epoch': 0.002575856829523854}\n",
      "{'loss': 0.7735, 'grad_norm': 0.6880066990852356, 'learning_rate': 1.7389201281818733e-05, 'epoch': 0.0026121365031791196}\n",
      "{'loss': 0.885, 'grad_norm': 0.5870690941810608, 'learning_rate': 1.7631053872664614e-05, 'epoch': 0.0026484161768343855}\n",
      "{'loss': 0.826, 'grad_norm': 0.6729239821434021, 'learning_rate': 1.7872906463510492e-05, 'epoch': 0.002684695850489651}\n",
      "{'loss': 0.6671, 'grad_norm': 0.9050096869468689, 'learning_rate': 1.811475905435637e-05, 'epoch': 0.0027209755241449163}\n",
      "{'loss': 0.752, 'grad_norm': 1.015957236289978, 'learning_rate': 1.835661164520225e-05, 'epoch': 0.002757255197800182}\n",
      "{'loss': 0.7173, 'grad_norm': 0.9906065464019775, 'learning_rate': 1.859846423604813e-05, 'epoch': 0.0027935348714554476}\n",
      "{'loss': 0.6882, 'grad_norm': 0.6418025493621826, 'learning_rate': 1.8840316826894007e-05, 'epoch': 0.002829814545110713}\n",
      "{'loss': 0.8489, 'grad_norm': 0.9755655527114868, 'learning_rate': 1.908216941773989e-05, 'epoch': 0.0028660942187659784}\n",
      "{'loss': 0.9031, 'grad_norm': 0.8625364303588867, 'learning_rate': 1.9324022008585767e-05, 'epoch': 0.0029023738924212443}\n",
      "{'loss': 0.7138, 'grad_norm': 0.854828417301178, 'learning_rate': 1.9565874599431648e-05, 'epoch': 0.0029386535660765097}\n",
      "{'loss': 0.6797, 'grad_norm': 1.1295201778411865, 'learning_rate': 1.980772719027753e-05, 'epoch': 0.002974933239731775}\n",
      "{'loss': 0.7977, 'grad_norm': 0.7480531930923462, 'learning_rate': 2.0049579781123407e-05, 'epoch': 0.003011212913387041}\n",
      "{'loss': 0.7345, 'grad_norm': 0.6240699291229248, 'learning_rate': 2.0291432371969285e-05, 'epoch': 0.0030474925870423064}\n",
      "{'loss': 0.7338, 'grad_norm': 1.2623755931854248, 'learning_rate': 2.0533284962815167e-05, 'epoch': 0.003083772260697572}\n",
      "{'loss': 0.656, 'grad_norm': 0.8901604413986206, 'learning_rate': 2.0775137553661045e-05, 'epoch': 0.0031200519343528377}\n",
      "{'loss': 0.617, 'grad_norm': 0.4633893072605133, 'learning_rate': 2.1016990144506922e-05, 'epoch': 0.003156331608008103}\n",
      "{'loss': 0.6797, 'grad_norm': 0.7399768829345703, 'learning_rate': 2.1258842735352804e-05, 'epoch': 0.0031926112816633686}\n",
      "{'loss': 0.8403, 'grad_norm': 0.9410005211830139, 'learning_rate': 2.1500695326198682e-05, 'epoch': 0.003228890955318634}\n",
      "{'loss': 0.7375, 'grad_norm': 0.857190728187561, 'learning_rate': 2.1742547917044563e-05, 'epoch': 0.0032651706289739}\n",
      "{'loss': 0.6873, 'grad_norm': 0.598443865776062, 'learning_rate': 2.1984400507890444e-05, 'epoch': 0.0033014503026291653}\n",
      "{'loss': 0.6609, 'grad_norm': 0.7028024792671204, 'learning_rate': 2.2226253098736322e-05, 'epoch': 0.0033377299762844307}\n",
      "{'loss': 0.7111, 'grad_norm': 0.618614912033081, 'learning_rate': 2.24681056895822e-05, 'epoch': 0.0033740096499396965}\n",
      "{'loss': 0.939, 'grad_norm': 0.46620285511016846, 'learning_rate': 2.270995828042808e-05, 'epoch': 0.003410289323594962}\n",
      "{'loss': 0.6462, 'grad_norm': 0.7795356512069702, 'learning_rate': 2.295181087127396e-05, 'epoch': 0.0034465689972502274}\n",
      "{'loss': 0.732, 'grad_norm': 0.585189938545227, 'learning_rate': 2.3193663462119838e-05, 'epoch': 0.0034828486709054932}\n",
      "{'loss': 0.6984, 'grad_norm': 0.6599765419960022, 'learning_rate': 2.343551605296572e-05, 'epoch': 0.0035191283445607587}\n",
      "{'loss': 0.6445, 'grad_norm': 0.7535637021064758, 'learning_rate': 2.3677368643811597e-05, 'epoch': 0.003555408018216024}\n",
      "{'loss': 0.7894, 'grad_norm': 0.5170367956161499, 'learning_rate': 2.3919221234657478e-05, 'epoch': 0.0035916876918712895}\n",
      "{'loss': 0.6946, 'grad_norm': 0.6036596894264221, 'learning_rate': 2.416107382550336e-05, 'epoch': 0.0036279673655265554}\n",
      "{'loss': 0.802, 'grad_norm': 0.4855666756629944, 'learning_rate': 2.4402926416349237e-05, 'epoch': 0.003664247039181821}\n",
      "{'loss': 0.7704, 'grad_norm': 0.6613206267356873, 'learning_rate': 2.4644779007195115e-05, 'epoch': 0.0037005267128370862}\n",
      "{'loss': 0.7428, 'grad_norm': 1.2494970560073853, 'learning_rate': 2.4886631598040997e-05, 'epoch': 0.003736806386492352}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# smoke test first if you want:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# trainer.train(max_steps=200)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Save LoRA adapter\u001b[39;00m\n\u001b[1;32m      7\u001b[0m adapter_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODELS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/mistral_ai_tutor_lora\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/transformers/trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2672\u001b[0m )\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2680\u001b[0m ):\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/transformers/trainer.py:4010\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4008\u001b[0m \u001b[38;5;66;03m# Context manager is no-op if CP isn't enabled\u001b[39;00m\n\u001b[1;32m   4009\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m cp_context():\n\u001b[0;32m-> 4010\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4011\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mtrain):\n\u001b[1;32m   4012\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:2891\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2889\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 2891\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2892\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:2891\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2889\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 2891\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2892\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/transformers/modeling_utils.py:5957\u001b[0m, in \u001b[0;36mPreTrainedModel.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   5956\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m-> 5957\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_kernels:\n\u001b[1;32m   5959\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernelize()\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:2891\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2889\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 2891\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2892\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/transformers/modeling_utils.py:5957\u001b[0m, in \u001b[0;36mPreTrainedModel.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   5956\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m-> 5957\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_kernels:\n\u001b[1;32m   5959\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernelize()\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:2891\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2889\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 2891\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2892\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:2891\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2889\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 2891\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2892\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: Module.train at line 2891 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:2891\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2889\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 2891\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2892\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:2889\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mode, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m   2888\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining mode is expected to be boolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2889\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m   2891\u001b[0m     module\u001b[38;5;241m.\u001b[39mtrain(mode)\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:2015\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   2013\u001b[0m             value \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m   2014\u001b[0m     modules[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m-> 2015\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   2016\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2017\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2018\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mtypename(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as child module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2019\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(torch.nn.Module or None expected)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2020\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# smoke test first if you want:\n",
    "# trainer.train(max_steps=200)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save LoRA adapter\n",
    "adapter_dir = f\"{MODELS_DIR}/mistral_ai_tutor_lora\"\n",
    "trainer.model.save_pretrained(adapter_dir)\n",
    "tokenizer.save_pretrained(adapter_dir)\n",
    "adapter_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e76aef5b-6dd0-4801-a7ac-0e1305923991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d118ce30f6634c1ebd7259bff607d228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model saved to: /home/cj.bowers/blue_cis6930/llama3_project/models/mistral_ai_tutor_merged\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch, pathlib\n",
    "\n",
    "# Merge on CPU to be VRAM-safe\n",
    "base_for_merge = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=None\n",
    ").to(\"cpu\")\n",
    "\n",
    "peft_loaded = PeftModel.from_pretrained(base_for_merge, adapter_dir)\n",
    "merged_model = peft_loaded.merge_and_unload()  # merged fp16 weights on CPU\n",
    "\n",
    "merged_dir = f\"{MODELS_DIR}/mistral_ai_tutor_merged\"\n",
    "pathlib.Path(merged_dir).mkdir(parents=True, exist_ok=True)\n",
    "merged_model.save_pretrained(merged_dir)\n",
    "tokenizer.save_pretrained(merged_dir)\n",
    "\n",
    "print(\"Merged model saved to:\", merged_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f51f3a78-7c67-466f-86fb-76459ec5acfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e928a2c6f1f74b7a9b1656968453bb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 22.05 GiB of which 58.12 MiB is free. Including non-PyTorch memory, this process has 21.98 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 404.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(eval_rows), B):\n\u001b[1;32m     12\u001b[0m     batch_prompts \u001b[38;5;241m=\u001b[39m [r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m eval_rows[i:i\u001b[38;5;241m+\u001b[39mB]]\n\u001b[0;32m---> 13\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrained_eval_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     preds_trained\u001b[38;5;241m.\u001b[39mextend(preds)\n\u001b[1;32m     16\u001b[0m trained_metrics \u001b[38;5;241m=\u001b[39m reduce_scores(preds_trained, golds)\n",
      "Cell \u001b[0;32mIn[4], line 60\u001b[0m, in \u001b[0;36mgenerate_batch\u001b[0;34m(prompts, model, max_new_tokens, temperature)\u001b[0m\n\u001b[1;32m     56\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     58\u001b[0m )\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 60\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m texts \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     69\u001b[0m completions \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/transformers/generation/utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2579\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/transformers/generation/utils.py:2784\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2781\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 2784\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2785\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:433\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    415\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[1;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/transformers/utils/generic.py:1064\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1064\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:369\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[0;32m--> 369\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    381\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    382\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:246\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    245\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 246\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:47\u001b[0m, in \u001b[0;36mMistralMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 47\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 22.05 GiB of which 58.12 MiB is free. Including non-PyTorch memory, this process has 21.98 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 404.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Load merged Mistral for eval\n",
    "trained_eval_model = AutoModelForCausalLM.from_pretrained(\n",
    "    merged_dir,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "# Compare baseline (already computed) vs trained\n",
    "preds_trained = []\n",
    "B = 4\n",
    "for i in range(0, len(eval_rows), B):\n",
    "    batch_prompts = [r[\"prompt\"] for r in eval_rows[i:i+B]]\n",
    "    preds = generate_batch(batch_prompts, trained_eval_model, max_new_tokens=256, temperature=0.0)\n",
    "    preds_trained.extend(preds)\n",
    "\n",
    "trained_metrics = reduce_scores(preds_trained, golds)\n",
    "baseline_metrics, trained_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7dbee8-31a5-421a-b66a-930e4f62ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, datetime\n",
    "\n",
    "ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_dir = f\"{OUTPUTS_DIR}/eval_{ts}\"\n",
    "pathlib.Path(results_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(f\"{results_dir}/metrics.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"baseline\": baseline_metrics,\n",
    "        \"trained\": trained_metrics,\n",
    "        \"n_eval\": len(golds),\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"adapter_dir\": adapter_dir,\n",
    "        \"merged_dir\": merged_dir\n",
    "    }, f, indent=2)\n",
    "\n",
    "with open(f\"{results_dir}/predictions.tsv\", \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f, delimiter=\"\\t\")\n",
    "    w.writerow([\"idx\",\"prompt\",\"gold\",\"baseline_pred\",\"trained_pred\"])\n",
    "    for i,(row,b,t) in enumerate(zip(eval_rows, preds_base, preds_trained)):\n",
    "        w.writerow([i,\n",
    "                    row[\"prompt\"].replace(\"\\n\",\" \")[:300],\n",
    "                    row[\"answer\"].replace(\"\\n\",\" \")[:300],\n",
    "                    b.replace(\"\\n\",\" \")[:300],\n",
    "                    t.replace(\"\\n\",\" \")[:300]])\n",
    "\n",
    "print(\"results saved to:\", results_dir)\n",
    "print(\"baseline:\", baseline_metrics)\n",
    "print(\"trained :\", trained_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8093e47a-7375-4c28-b37a-0eb9e933af8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading BASE model (no LoRA)…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d178298f7464403c9243beed0b04659b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 22.05 GiB of which 58.12 MiB is free. Including non-PyTorch memory, this process has 21.98 GiB memory in use. Of the allocated memory 21.50 GiB is allocated by PyTorch, and 247.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 63\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# ================== 1) BASE MODEL ONLY ==================\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading BASE model (no LoRA)…\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m---> 63\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m base_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     66\u001b[0m base_outputs \u001b[38;5;241m=\u001b[39m run_model(base_model, TEST_PROMPTS, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBASE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/transformers/modeling_utils.py:4343\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   4339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4340\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4341\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4342\u001b[0m         )\n\u001b[0;32m-> 4343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:1371\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 930\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    934\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    941\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 930\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    934\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    941\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 930 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 930\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    934\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    941\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:957\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 957\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    958\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "File \u001b[0;32m~/.conda/envs/llama3/lib/python3.10/site-packages/torch/nn/modules/module.py:1357\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1351\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1352\u001b[0m             device,\n\u001b[1;32m   1353\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1354\u001b[0m             non_blocking,\n\u001b[1;32m   1355\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1356\u001b[0m         )\n\u001b[0;32m-> 1357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1363\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 22.05 GiB of which 58.12 MiB is free. Including non-PyTorch memory, this process has 21.98 GiB memory in use. Of the allocated memory 21.50 GiB is allocated by PyTorch, and 247.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# ----- CONFIG -----\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"  # or your base model\n",
    "ADAPTER_DIR = f\"{OUTPUTS_DIR}/mistral_ai_tutor_lora\"\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    \"Explain what a function is in Python to a first-year CS student.\",\n",
    "    \"Help me understand the difference between a list and a dictionary in Python.\",\n",
    "    \"I keep forgetting the chain rule in calculus. Can you explain it with an example?\",\n",
    "    \"I got 75% on a quiz. It had 40 questions. How many did I get right?\",\n",
    "]\n",
    "\n",
    "MAX_NEW_TOKENS = 200\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 0.9\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Use tokenizer from your adapter folder (includes special tokens + template)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    ADAPTER_DIR,\n",
    "    use_fast=True,\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "\n",
    "def run_model(model, prompts, label):\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    outputs = []\n",
    "    for i, prompt in enumerate(prompts, start=1):\n",
    "        print(f\"\\n[{label}] Prompt {i}/{len(prompts)}\")\n",
    "        print(\"Q:\", prompt)\n",
    "\n",
    "        full_prompt = f\"###Human: {prompt}\\n###Assistant:\"\n",
    "        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                temperature=TEMPERATURE,\n",
    "                top_p=TOP_P,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        text = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "        outputs.append(text)\n",
    "        print(\"---- Response ----\")\n",
    "        print(text)\n",
    "        print(\"------------------\")\n",
    "    return outputs\n",
    "\n",
    "\n",
    "# ================== 1) BASE MODEL ONLY ==================\n",
    "print(\"Loading BASE model (no LoRA)…\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)\n",
    "base_model.eval()\n",
    "\n",
    "base_outputs = run_model(base_model, TEST_PROMPTS, \"BASE\")\n",
    "\n",
    "del base_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# ================== 2) TUNED MODEL (LoRA) ==================\n",
    "print(\"\\nLoading TUNED model (Mistral + LoRA)…\")\n",
    "tuned_base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)\n",
    "\n",
    "tuned_model = PeftModel.from_pretrained(tuned_base, ADAPTER_DIR)\n",
    "tuned_model.eval()\n",
    "\n",
    "tuned_outputs = run_model(tuned_model, TEST_PROMPTS, \"TUNED\")\n",
    "\n",
    "# ================== 3) SIDE-BY-SIDE SUMMARY ==================\n",
    "print(\"\\n\\n================ SIDE-BY-SIDE COMPARISON ================\\n\")\n",
    "for i, prompt in enumerate(TEST_PROMPTS):\n",
    "    print(f\"### Prompt {i+1}\")\n",
    "    print(\"Q:\", prompt)\n",
    "    print(\"\\n[BASE]\")\n",
    "    print(base_outputs[i])\n",
    "    print(\"\\n[TUNED]\")\n",
    "    print(tuned_outputs[i])\n",
    "    print(\"\\n\" + \"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7109dbd-16d3-47c7-9f83-2214b0cae358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3 (PyTorch CUDA)",
   "language": "python",
   "name": "llama3-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
