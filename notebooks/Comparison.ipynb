{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c53e55fd-7ea8-42d3-9569-8137fda8bb70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded 4410183 eval examples from /blue/cis6930/cj.bowers/llama3_project/data/ai_tutor_merged.jsonl\n",
      "Using 400 random examples for evaluation.\n",
      "\n",
      "\n",
      "[LOAD] Base model (4-bit, no LoRA)…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229888f61b9b4de6bfd50f0bed1709d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generating 1/400…\n",
      "  Generating 10/400…\n",
      "  Generating 20/400…\n",
      "  Generating 30/400…\n",
      "  Generating 40/400…\n",
      "  Generating 50/400…\n",
      "  Generating 60/400…\n",
      "  Generating 70/400…\n",
      "  Generating 80/400…\n",
      "  Generating 90/400…\n",
      "  Generating 100/400…\n",
      "  Generating 110/400…\n",
      "  Generating 120/400…\n",
      "  Generating 130/400…\n",
      "  Generating 140/400…\n",
      "  Generating 150/400…\n",
      "  Generating 160/400…\n",
      "  Generating 170/400…\n",
      "  Generating 180/400…\n",
      "  Generating 190/400…\n",
      "  Generating 200/400…\n",
      "  Generating 210/400…\n",
      "  Generating 220/400…\n",
      "  Generating 230/400…\n",
      "  Generating 240/400…\n",
      "  Generating 250/400…\n",
      "  Generating 260/400…\n",
      "  Generating 270/400…\n",
      "  Generating 280/400…\n",
      "  Generating 290/400…\n",
      "  Generating 300/400…\n",
      "  Generating 310/400…\n",
      "  Generating 320/400…\n",
      "  Generating 330/400…\n",
      "  Generating 340/400…\n",
      "  Generating 350/400…\n",
      "  Generating 360/400…\n",
      "  Generating 370/400…\n",
      "  Generating 380/400…\n",
      "  Generating 390/400…\n",
      "  Generating 400/400…\n",
      "\n",
      "[RESULT] Base model mean F1: 0.294\n",
      "\n",
      "[LOAD] Tuned model (4-bit + LoRA)…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867b6c7bbd454ac18b01b5700575f0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generating 1/400…\n",
      "  Generating 10/400…\n",
      "  Generating 20/400…\n",
      "  Generating 30/400…\n",
      "  Generating 40/400…\n",
      "  Generating 50/400…\n",
      "  Generating 60/400…\n",
      "  Generating 70/400…\n",
      "  Generating 80/400…\n",
      "  Generating 90/400…\n",
      "  Generating 100/400…\n",
      "  Generating 110/400…\n",
      "  Generating 120/400…\n",
      "  Generating 130/400…\n",
      "  Generating 140/400…\n",
      "  Generating 150/400…\n",
      "  Generating 160/400…\n",
      "  Generating 170/400…\n",
      "  Generating 180/400…\n",
      "  Generating 190/400…\n",
      "  Generating 200/400…\n",
      "  Generating 210/400…\n",
      "  Generating 220/400…\n",
      "  Generating 230/400…\n",
      "  Generating 240/400…\n",
      "  Generating 250/400…\n",
      "  Generating 260/400…\n",
      "  Generating 270/400…\n",
      "  Generating 280/400…\n",
      "  Generating 290/400…\n",
      "  Generating 300/400…\n",
      "  Generating 310/400…\n",
      "  Generating 320/400…\n",
      "  Generating 330/400…\n",
      "  Generating 340/400…\n",
      "  Generating 350/400…\n",
      "  Generating 360/400…\n",
      "  Generating 370/400…\n",
      "  Generating 380/400…\n",
      "  Generating 390/400…\n",
      "  Generating 400/400…\n",
      "[RESULT] Tuned model mean F1: 0.329\n",
      "\n",
      "Plots saved to: /blue/cis6930/cj.bowers/llama3_project/outputs/plots\n",
      "  - mean_f1_base_vs_tuned.png\n",
      "  - improvement_histogram.png\n",
      "  - f1_boxplot_base_vs_tuned.png\n",
      "  - f1_scatter_base_vs_tuned.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/18274227/ipykernel_1076185/3125690621.py:267: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "eval_compare_mistral.py\n",
    "\n",
    "Compare base Mistral vs LoRA-tuned Mistral on a small eval set and\n",
    "produce plots for a presentation.\n",
    "\n",
    "Requires:\n",
    "  pip install --upgrade torch transformers accelerate bitsandbytes datasets matplotlib peft\n",
    "\"\"\"\n",
    "\n",
    "# =========================================================\n",
    "# 1. CONFIG & IMPORTS\n",
    "# =========================================================\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import PeftModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------- EDIT THESE PATHS / SETTINGS AS NEEDED ------------------\n",
    "\n",
    "# Base model you fine-tuned\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Folder with your exported LoRA adapter (the one we just created)\n",
    "ADAPTER_DIR = \"/blue/cis6930/cj.bowers/llama3_project/outputs/mistral_ai_tutor_lora\"\n",
    "\n",
    "# Eval set: JSONL with the *same format as your training data*:\n",
    "#   {\"text\": \"###System: ... ###Human: ... ###Assistant: ideal answer ...\"}\n",
    "EVAL_JSONL = \"/blue/cis6930/cj.bowers/llama3_project/data/ai_tutor_merged.jsonl\"\n",
    "\n",
    "# How many examples to evaluate on (random subset)\n",
    "MAX_EVAL_EXAMPLES = 400  # set to 300–500 as you like\n",
    "\n",
    "# Where to save plots\n",
    "PLOTS_DIR = \"/blue/cis6930/cj.bowers/llama3_project/outputs/plots\"\n",
    "\n",
    "# Generation settings\n",
    "MAX_NEW_TOKENS = 160\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 0.9\n",
    "\n",
    "# Marker used in your data to separate prompt from answer\n",
    "ASSISTANT_MARK = \"###Assistant:\"\n",
    "\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =========================================================\n",
    "# 2. HELPER: LOAD EVAL DATA & SPLIT PROMPT / REFERENCE\n",
    "# =========================================================\n",
    "\n",
    "def load_eval_examples(path: str):\n",
    "    \"\"\"\n",
    "    Load JSONL with field 'text' and split into (prompt, reference_answer)\n",
    "    using ASSISTANT_MARK, same as training format.\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\"json\", data_files=path, split=\"train\")\n",
    "    prompts = []\n",
    "    refs = []\n",
    "\n",
    "    for row in ds:\n",
    "        text = row[\"text\"]\n",
    "        if ASSISTANT_MARK in text:\n",
    "            before, after = text.split(ASSISTANT_MARK, 1)\n",
    "            prompt = before.strip()\n",
    "            ref = after.strip()\n",
    "        else:\n",
    "            # fallback: whole text is prompt, reference empty\n",
    "            prompt = text.strip()\n",
    "            ref = \"\"\n",
    "        prompts.append(prompt)\n",
    "        refs.append(ref)\n",
    "\n",
    "    print(f\"Loaded {len(prompts)} eval examples from {path}\")\n",
    "    return prompts, refs\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. HELPER: SIMPLE TOKEN-LEVEL F1 SCORE\n",
    "# =========================================================\n",
    "\n",
    "def token_f1(pred: str, ref: str) -> float:\n",
    "    \"\"\"\n",
    "    Very simple F1 on whitespace tokens; not perfect but fine for comparison.\n",
    "    \"\"\"\n",
    "    pred_tokens = pred.lower().split()\n",
    "    ref_tokens = ref.lower().split()\n",
    "    if not ref_tokens:\n",
    "        return 0.0\n",
    "    pred_set = set(pred_tokens)\n",
    "    ref_set = set(ref_tokens)\n",
    "    inter = pred_set & ref_set\n",
    "    if not inter:\n",
    "        return 0.0\n",
    "    precision = len(inter) / len(pred_set) if pred_set else 0.0\n",
    "    recall = len(inter) / len(ref_set) if ref_set else 0.0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. MODEL LOADING HELPERS (4-BIT TO FIT ON L4)\n",
    "# =========================================================\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def load_base_model():\n",
    "    print(\"\\n[LOAD] Base model (4-bit, no LoRA)…\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},   # all modules on GPU 0 in 4-bit\n",
    "    )\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_tuned_model():\n",
    "    print(\"\\n[LOAD] Tuned model (4-bit + LoRA)…\")\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base, ADAPTER_DIR)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. GENERATION LOOP\n",
    "# =========================================================\n",
    "\n",
    "def generate_answers(model, tokenizer, prompts: List[str]) -> List[str]:\n",
    "    answers = []\n",
    "    for i, prompt_text in enumerate(prompts, start=1):\n",
    "        if i % 10 == 0 or i == 1:\n",
    "            print(f\"  Generating {i}/{len(prompts)}…\")\n",
    "\n",
    "        # prompt_text already contains '###Human:' etc from your data\n",
    "        full_prompt = prompt_text + \"\\n\" + ASSISTANT_MARK\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            full_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                temperature=TEMPERATURE,\n",
    "                top_p=TOP_P,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        text = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "        # extract part after ASSISTANT_MARK as model's answer\n",
    "        if ASSISTANT_MARK in text:\n",
    "            _, answer = text.split(ASSISTANT_MARK, 1)\n",
    "        else:\n",
    "            answer = text\n",
    "        answers.append(answer.strip())\n",
    "    return answers\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. MAIN EVALUATION LOGIC\n",
    "# =========================================================\n",
    "\n",
    "def main():\n",
    "    # ----- load eval data -----\n",
    "    prompts, refs = load_eval_examples(EVAL_JSONL)\n",
    "\n",
    "    # ---- random subset selection ----\n",
    "    combined = list(zip(prompts, refs))\n",
    "    random.seed(42)  # reproducible\n",
    "    random.shuffle(combined)\n",
    "    n = min(MAX_EVAL_EXAMPLES, len(combined))\n",
    "    combined = combined[:n]\n",
    "    prompts, refs = zip(*combined)\n",
    "    prompts, refs = list(prompts), list(refs)\n",
    "    print(f\"Using {n} random examples for evaluation.\\n\")\n",
    "\n",
    "    # tokenizer from adapter dir (includes your special tokens)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        ADAPTER_DIR,\n",
    "        use_fast=True,\n",
    "        padding_side=\"left\",\n",
    "    )\n",
    "\n",
    "    # ----- BASE MODEL -----\n",
    "    base_model = load_base_model()\n",
    "    base_answers = generate_answers(base_model, tokenizer, prompts)\n",
    "    base_scores = [token_f1(p, r) for p, r in zip(base_answers, refs)]\n",
    "    base_mean = float(sum(base_scores) / len(base_scores))\n",
    "    print(f\"\\n[RESULT] Base model mean F1: {base_mean:.3f}\")\n",
    "\n",
    "    del base_model\n",
    "    cleanup()\n",
    "\n",
    "    # ----- TUNED MODEL -----\n",
    "    tuned_model = load_tuned_model()\n",
    "    tuned_answers = generate_answers(tuned_model, tokenizer, prompts)\n",
    "    tuned_scores = [token_f1(p, r) for p, r in zip(tuned_answers, refs)]\n",
    "    tuned_mean = float(sum(tuned_scores) / len(tuned_scores))\n",
    "    print(f\"[RESULT] Tuned model mean F1: {tuned_mean:.3f}\")\n",
    "\n",
    "    del tuned_model\n",
    "    cleanup()\n",
    "\n",
    "    # =====================================================\n",
    "    # 7. PLOTS FOR PRESENTATION\n",
    "    # =====================================================\n",
    "    import numpy as np\n",
    "\n",
    "    improvements = [t - b for b, t in zip(tuned_scores, base_scores)]\n",
    "\n",
    "    # --- 1) Bar chart of average scores ---\n",
    "    plt.figure()\n",
    "    plt.bar([\"Base\", \"Tuned\"], [base_mean, tuned_mean])\n",
    "    plt.ylabel(\"Mean token F1 vs reference\")\n",
    "    plt.title(f\"Base vs LoRA-tuned Mistral (mean F1 on {n} examples)\")\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, \"mean_f1_base_vs_tuned.png\"), bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # --- 2) Histogram of per-example improvement ---\n",
    "    plt.figure()\n",
    "    plt.hist(improvements, bins=20)\n",
    "    plt.xlabel(\"F1(Tuned) - F1(Base)\")\n",
    "    plt.ylabel(\"# examples\")\n",
    "    plt.title(\"Per-example improvement (positive = tuned is better)\")\n",
    "    plt.axvline(0.0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, \"improvement_histogram.png\"), bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # --- 3) Boxplot of F1 distributions ---\n",
    "    plt.figure()\n",
    "    plt.boxplot(\n",
    "        [base_scores, tuned_scores],\n",
    "        labels=[\"Base\", \"Tuned\"],\n",
    "        showmeans=True,\n",
    "    )\n",
    "    plt.ylabel(\"Token F1\")\n",
    "    plt.title(\"Distribution of F1 scores (Base vs Tuned)\")\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, \"f1_boxplot_base_vs_tuned.png\"), bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # --- 4) Scatter: base vs tuned F1 ---\n",
    "    plt.figure()\n",
    "    plt.scatter(base_scores, tuned_scores, alpha=0.5, s=10)\n",
    "    plt.xlabel(\"Base F1\")\n",
    "    plt.ylabel(\"Tuned F1\")\n",
    "    plt.title(\"Per-example F1: Base vs Tuned\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")  # diagonal line\n",
    "    plt.xlim(0, 1.0)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, \"f1_scatter_base_vs_tuned.png\"), bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"\\nPlots saved to: {PLOTS_DIR}\")\n",
    "    print(\"  - mean_f1_base_vs_tuned.png\")\n",
    "    print(\"  - improvement_histogram.png\")\n",
    "    print(\"  - f1_boxplot_base_vs_tuned.png\")\n",
    "    print(\"  - f1_scatter_base_vs_tuned.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a47b706-4d01-4ae9-8a46-63911c72df05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3 (PyTorch CUDA)",
   "language": "python",
   "name": "llama3-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
